"""
Markdown report generation for the loss landscape project.

This module reads experiment metadata and metrics from the
``reports/experiments/`` hierarchy and generates high-level
Markdown summaries linking to generated figures.
"""

from __future__ import annotations

import json
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, List, Tuple


@dataclass
class RunSummary:
    """
    Lightweight container for a single training run summary.

    Args:
        dataset (str): Dataset name.
        architecture (str): Architecture identifier.
        activation (str): Activation function name.
        optimizer (str): Optimizer name.
        seed (int): Random seed used.
        final_train_loss (float): Final training loss.
        final_test_loss (float): Final test loss.
        final_test_accuracy (float): Final test accuracy.
    """

    dataset: str
    architecture: str
    activation: str
    optimizer: str
    seed: int
    final_train_loss: float
    final_test_loss: float
    final_test_accuracy: float


def _load_run_summaries(experiments_root: Path) -> List[RunSummary]:
    """
    Load run summaries from metrics JSON files under an experiments root.

    Args:
        experiments_root (Path): Root directory containing experiment runs.

    Returns:
        List[RunSummary]: List of run summaries.
    """
    summaries: List[RunSummary] = []

    if not experiments_root.exists():
        return summaries

    for metrics_path in experiments_root.rglob("metrics.json"):
        run_dir = metrics_path.parent
        summary_path = run_dir / "summary.json"
        if not summary_path.exists():
            continue

        with metrics_path.open("r", encoding="utf-8") as f_metrics:
            metrics = json.load(f_metrics)
        if not metrics:
            continue
        last_epoch = metrics[-1]

        with summary_path.open("r", encoding="utf-8") as f_summary:
            summary_list = json.load(f_summary)
        if not summary_list:
            continue
        summary = summary_list[0]

        dataset_name = summary["dataset"]["name"]
        arch = summary["model"]["hidden_layers"]
        hidden_size = summary["model"]["hidden_size"]
        activation = summary["model"]["activation"]
        optimizer = summary["training"]["optimizer"]
        seed = int(summary["seed"])

        summaries.append(
            RunSummary(
                dataset=dataset_name,
                architecture=f"{arch}x{hidden_size}",
                activation=activation,
                optimizer=optimizer,
                seed=seed,
                final_train_loss=float(last_epoch["train_loss"]),
                final_test_loss=float(last_epoch["test_loss"]),
                final_test_accuracy=float(last_epoch["test_accuracy"]),
            )
        )

    return summaries


def _summaries_to_markdown(summaries: List[RunSummary]) -> str:
    """
    Convert a collection of run summaries into a Markdown report string.

    Args:
        summaries (List[RunSummary]): Collection of run summaries.

    Returns:
        str: Markdown document content.
    """
    lines: List[str] = []

    lines.append("# Loss Landscape Geometry â€” Experiment Summary")
    lines.append("")
    lines.append("## Overview")
    lines.append("")
    lines.append("This report summarizes trained models and final performance metrics ")
    lines.append("for the current experiment matrix. Figures generated by the ")
    lines.append("visualization utilities can be referenced from the respective ")
    lines.append("experiment directories under `reports/figures/`.")
    lines.append("")

    if not summaries:
        lines.append("_No experiment runs were found._")
        return "\n".join(lines)

    lines.append("## Model Performance Summary")
    lines.append("")
    lines.append("| Dataset | Architecture | Activation | Optimizer | Seed | Train Loss | Test Loss | Test Accuracy |")
    lines.append("| ------- | ------------ | ---------- | --------- | ---- | ---------- | --------- | ------------- |")

    for s in summaries:
        lines.append(
            f"| {s.dataset} | {s.architecture} | {s.activation} | {s.optimizer} | "
            f"{s.seed} | {s.final_train_loss:.4f} | {s.final_test_loss:.4f} | {s.final_test_accuracy:.4f} |"
        )

    lines.append("")
    lines.append("## Next Steps")
    lines.append("")
    lines.append(
        "- Use Hessian, sharpness, and connectivity modules to populate "
        "additional sections such as curvature analysis and mode connectivity."
    )

    return "\n".join(lines)


def generate_summary_report(
    experiments_root: Path,
    report_path: Path,
) -> None:
    """
    Generate a top-level Markdown summary report from experiment metrics.

    Args:
        experiments_root (Path): Root directory containing experiment runs
            (e.g. `reports/experiments`).
        report_path (Path): Target file path for `reports/summary.md`.

    Returns:
        None
    """
    summaries = _load_run_summaries(experiments_root)
    markdown = _summaries_to_markdown(summaries)
    report_path.parent.mkdir(parents=True, exist_ok=True)
    with report_path.open("w", encoding="utf-8") as f:
        f.write(markdown)


def _write_report(report_path: Path, content: str) -> None:
    """
    Write Markdown content to a file, creating parent directories as needed.

    Args:
        report_path (Path): Destination file path.
        content (str): Markdown content to write.

    Returns:
        None
    """
    report_path.parent.mkdir(parents=True, exist_ok=True)
    with report_path.open("w", encoding="utf-8") as f:
        f.write(content)


def _mean(values: List[float]) -> float:
    """
    Compute the arithmetic mean of a list of floats.

    Args:
        values (List[float]): Values to average.

    Returns:
        float: Mean value or 0.0 if the list is empty.
    """
    if not values:
        return 0.0
    return float(sum(values) / len(values))


def _group_by_key(
    summaries: List[RunSummary],
    key: str,
) -> Dict[str, List[RunSummary]]:
    """
    Group run summaries by a specified attribute name.

    Args:
        summaries (List[RunSummary]): Collection of summaries.
        key (str): Attribute name of ``RunSummary`` to group by.

    Returns:
        Dict[str, List[RunSummary]]: Mapping from attribute value to runs.
    """
    groups: Dict[str, List[RunSummary]] = {}
    for s in summaries:
        group_key = getattr(s, key)
        groups.setdefault(str(group_key), []).append(s)
    return groups


def _load_connectivity_stats(figures_root: Path) -> Dict[str, Dict[str, object]]:
    """
    Load connectivity barrier statistics from figures directory.

    This scans ``reports/figures/.../connectivity/*/barriers.json`` and
    aggregates train/test barrier heights per (dataset, architecture,
    activation, optimizer) configuration.

    Args:
        figures_root (Path): Root directory containing connectivity figures.

    Returns:
        Dict[str, Dict[str, object]]: Mapping from a configuration key to
        metadata and lists of barrier values.
    """
    stats: Dict[str, Dict[str, object]] = {}

    if not figures_root.exists():
        return stats

    for path in figures_root.rglob("barriers.json"):
        try:
            with path.open("r", encoding="utf-8") as f_bar:
                data = json.load(f_bar)
        except Exception:
            continue

        train_barrier = float(data.get("train_barrier", 0.0))
        test_barrier = float(data.get("test_barrier", 0.0))

        pair_dir = path.parent
        connectivity_dir = pair_dir.parent
        opt_dir = connectivity_dir.parent
        act_dir = opt_dir.parent
        arch_dir = act_dir.parent
        dataset_dir = arch_dir.parent

        def _value(name: str) -> str:
            return name.split("=", 1)[1] if "=" in name else name

        dataset = _value(dataset_dir.name)
        arch = _value(arch_dir.name)
        activation = _value(act_dir.name)
        optimizer = _value(opt_dir.name)

        key = "|".join([dataset, arch, activation, optimizer])
        entry = stats.setdefault(
            key,
            {
                "dataset": dataset,
                "architecture": arch,
                "activation": activation,
                "optimizer": optimizer,
                "train_barriers": [],
                "test_barriers": [],
            },
        )

        train_list: List[float] = entry["train_barriers"]  # type: ignore[assignment]
        test_list: List[float] = entry["test_barriers"]  # type: ignore[assignment]
        train_list.append(train_barrier)
        test_list.append(test_barrier)

    return stats


def _study_report_template(
    title: str,
    description: str,
    group_label: str,
    groups: Dict[str, List[RunSummary]],
) -> str:
    """
    Build a generic study report as Markdown.

    Args:
        title (str): Report title.
        description (str): Short description of the study.
        group_label (str): Column label for the grouping dimension.
        groups (Dict[str, List[RunSummary]]): Grouped run summaries.

    Returns:
        str: Markdown document content.
    """
    lines: List[str] = []

    lines.append(f"# {title}")
    lines.append("")
    lines.append("## Overview")
    lines.append("")
    lines.append(description)
    lines.append("")
    lines.append("## Model performance summary")
    lines.append("")
    lines.append(
        f"| {group_label} | Dataset(s) | Activation(s) | Optimizer(s) | "
        "Mean Test Loss | Mean Test Accuracy |"
    )
    lines.append(
        "| --------- | ---------- | ------------- | ----------- | "
        "-------------- | ------------------- |"
    )

    if not groups:
        lines.append("| _none_ | - | - | - | - | - |")
        return "\n".join(lines)

    for key, runs in sorted(groups.items()):
        datasets = sorted({r.dataset for r in runs})
        activations = sorted({r.activation for r in runs})
        optimizers = sorted({r.optimizer for r in runs})
        mean_test_loss = _mean([r.final_test_loss for r in runs])
        mean_test_acc = _mean([r.final_test_accuracy for r in runs])

        lines.append(
            f"| {key} | {', '.join(datasets)} | {', '.join(activations)} | "
            f"{', '.join(optimizers)} | {mean_test_loss:.4f} | {mean_test_acc:.4f} |"
        )

    return "\n".join(lines)


def generate_study_reports(
    experiments_root: Path,
    reports_root: Path,
) -> None:
    """
    Generate detailed Markdown study reports for different comparison axes.

    This function writes the following files under ``reports_root``:

        - ``summary.md`` (via ``generate_summary_report``)
        - ``depth_study.md``
        - ``width_study.md``
        - ``activation_study.md``
        - ``optimizer_study.md``
        - ``connectivity_study.md`` (aggregated connectivity statistics)

    Args:
        experiments_root (Path): Root directory containing experiment runs.
        reports_root (Path): Directory where report Markdown files are stored.

    Returns:
        None
    """
    summaries = _load_run_summaries(experiments_root)

    # Top-level summary.
    generate_summary_report(experiments_root, reports_root / "summary.md")

    if not summaries:
        empty_content = "_No experiment runs were found for detailed studies._"
        _write_report(reports_root / "depth_study.md", empty_content)
        _write_report(reports_root / "width_study.md", empty_content)
        _write_report(reports_root / "activation_study.md", empty_content)
        _write_report(reports_root / "optimizer_study.md", empty_content)
        _write_report(
            reports_root / "connectivity_study.md",
            "_No connectivity experiments were found._",
        )
        return

    # Depth and width are encoded in the "architecture" field as "<layers>x<hidden>".
    depth_groups: Dict[str, List[RunSummary]] = {}
    width_groups: Dict[str, List[RunSummary]] = {}
    for s in summaries:
        try:
            layers_str, width_str = s.architecture.split("x")
        except ValueError:
            layers_str, width_str = "unknown", "unknown"
        depth_groups.setdefault(layers_str, []).append(s)
        width_groups.setdefault(width_str, []).append(s)

    depth_content = _study_report_template(
        title="Depth Study",
        description=(
            "This report compares models grouped by depth (number of hidden layers) "
            "using final test performance statistics."
        ),
        group_label="Hidden Layers",
        groups=depth_groups,
    )
    _write_report(reports_root / "depth_study.md", depth_content)

    width_content = _study_report_template(
        title="Width Study",
        description=(
            "This report compares models grouped by hidden layer width using "
            "final test performance statistics."
        ),
        group_label="Hidden Size",
        groups=width_groups,
    )
    _write_report(reports_root / "width_study.md", width_content)

    activation_groups = _group_by_key(summaries, "activation")
    activation_content = _study_report_template(
        title="Activation Study",
        description=(
            "This report compares activation functions (ReLU, Tanh, GELU) across "
            "architectures and optimizers using final test metrics."
        ),
        group_label="Activation",
        groups=activation_groups,
    )
    _write_report(reports_root / "activation_study.md", activation_content)

    optimizer_groups = _group_by_key(summaries, "optimizer")
    optimizer_content = _study_report_template(
        title="Optimizer Study",
        description=(
            "This report compares SGD and Adam optimizers across datasets, "
            "architectures, and activations."
        ),
        group_label="Optimizer",
        groups=optimizer_groups,
    )
    _write_report(reports_root / "optimizer_study.md", optimizer_content)

    connectivity_lines: List[str] = []
    connectivity_lines.append("# Connectivity Study")
    connectivity_lines.append("")
    connectivity_lines.append("## Overview")
    connectivity_lines.append("")
    connectivity_lines.append(
        "Connectivity experiments (linear and curved paths between modes) are "
        "summarized via barrier-height annotated plots saved under "
        "`reports/figures/.../connectivity/` for each (dataset, architecture, "
        "activation, optimizer, seed-pair) combination."
    )
    connectivity_lines.append("")

    figures_root = reports_root / "figures"
    connectivity_stats = _load_connectivity_stats(figures_root)

    if not connectivity_stats:
        connectivity_lines.append(
            "_No connectivity barrier JSON files were found. "
            "Ensure probes have been run to completion to populate "
            "`reports/figures/.../connectivity/*/barriers.json`._"
        )
        _write_report(reports_root / "connectivity_study.md", "\n".join(connectivity_lines))
        return

    connectivity_lines.append("## Connectivity summary")
    connectivity_lines.append("")
    connectivity_lines.append(
        "| Dataset | Architecture | Activation | Optimizer | "
        "Num Pairs | Mean Train Barrier | Mean Test Barrier | "
        "Max Train Barrier | Max Test Barrier |"
    )
    connectivity_lines.append(
        "| ------- | ------------ | ---------- | --------- | "
        "--------- | ------------------- | ------------------ | "
        "------------------- | ------------------ |"
    )

    for key in sorted(connectivity_stats.keys()):
        entry = connectivity_stats[key]
        dataset = entry["dataset"]  # type: ignore[assignment]
        arch = entry["architecture"]  # type: ignore[assignment]
        activation = entry["activation"]  # type: ignore[assignment]
        optimizer = entry["optimizer"]  # type: ignore[assignment]
        train_vals: List[float] = entry["train_barriers"]  # type: ignore[assignment]
        test_vals: List[float] = entry["test_barriers"]  # type: ignore[assignment]

        if train_vals and test_vals:
            num_pairs = len(train_vals)
            mean_train = _mean(train_vals)
            mean_test = _mean(test_vals)
            max_train = max(train_vals)
            max_test = max(test_vals)
        else:
            num_pairs = 0
            mean_train = 0.0
            mean_test = 0.0
            max_train = 0.0
            max_test = 0.0

        connectivity_lines.append(
            f"| {dataset} | {arch} | {activation} | {optimizer} | "
            f"{num_pairs} | {mean_train:.4f} | {mean_test:.4f} | "
            f"{max_train:.4f} | {max_test:.4f} |"
        )

    _write_report(reports_root / "connectivity_study.md", "\n".join(connectivity_lines))
